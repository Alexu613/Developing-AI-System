{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing AI System\n",
    "\n",
    "### Chatbot\n",
    "\n",
    "A **chatbot** is a computer program that simulates human conversation, usually through text or voice. With **OpenAI**, a chatbot uses powerful language models like **GPT-4** to understand questions and generate smart, human-like answers.\n",
    "\n",
    "In simple terms:\n",
    "\n",
    "> A chatbot with OpenAI can chat, answer questions, summarize info, or help with tasks ‚Äî almost like talking to a helpful assistant.\n",
    "\n",
    "It works by sending messages to the model (input) and receiving a response (output), often using the `chat.completions.create()` function from the OpenAI API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexBot is ready. Tape 'exit' to quit the conversation\n",
      "AlexBot : Hello! How can I assist you today?\n",
      "--------------------------------------------------\n",
      "Thanks for choosing AlexBot. See you soon !\n"
     ]
    }
   ],
   "source": [
    "#This version of chatbot can only be running through the console,\n",
    "#Check code app.py a version of this chatbot deployed on Streamlit Platform for a better experience\n",
    "#To run the chatbot --> Terminal : streamlit run app.py\n",
    "\n",
    "import os \n",
    "import openai\n",
    "from time import sleep\n",
    "from tqdm import tgrange, tqdm_notebook\n",
    "\n",
    "#Bloc\n",
    "MESSAGES = [{\"role\":\"system\", \"content\": \"You're a Chatbot, your name is AlexBot you've been created by Alexandre Ohayon, you can answer any customer's questions by summarizing your answer.\"}]\n",
    "#Dialog Box ask your question to the ChatBot:\n",
    "\n",
    "try:\n",
    "    print(\"AlexBot is ready. Tape 'exit' to quit the conversation\")\n",
    "\n",
    "    #Start a session:\n",
    "    while True:\n",
    "        #Enter your question:\n",
    "        USER_PROMPT = input(\"Enter your message here: \") #Use the dialog box\n",
    "        #Tape exist in lower character to quit the conversation with the Bot.\n",
    "        if USER_PROMPT.lower() == \"exit\": \n",
    "            print(\"Thanks for choosing AlexBot. See you soon !\") #Message from system\n",
    "            break #Session closed\n",
    "            \n",
    "        #If a conversation exists then add user question to the Bloc in line 7:\n",
    "        MESSAGES.append({\"role\":\"user\", \"content\": USER_PROMPT}) #Add the question into the list:\n",
    "        user_question = {\"role\":\"user\", \"content\": USER_PROMPT}\n",
    "        #print(f\"User_question : {USER_PROMPT}\")\n",
    "\n",
    "                \n",
    "        #Pass MESSAGES through the LLM to get response from the AI Agent:\n",
    "        client = openai.OpenAI(organization=None,\n",
    "                                project=None, \n",
    "                                timeout=60*5, #5 minutes and break after that\n",
    "                                max_retries=2,\n",
    "                                api_key=os.getenv(\"API_KEY\"))\n",
    "            \n",
    "        #Response from the Intelligent Agent:\n",
    "        # Add the appropriate parameters to the decorator\n",
    "        response = client.chat.completions.create(\n",
    "            model='gpt-4o-mini',\n",
    "            messages=MESSAGES,\n",
    "            max_tokens=300,\n",
    "            temperature=0,\n",
    "            top_p=0.9,\n",
    "            stop=[\"\\n\"]\n",
    "        )\n",
    "\n",
    "        assistant_dict = {\"role\":\"assistant\", \"content\": response.choices[0].message.content}\n",
    "        print(f\"AlexBot : {assistant_dict[\"content\"]}\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "        #Add the question into the list:\n",
    "        MESSAGES.append(assistant_dict)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error :\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get response in Json Format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"plats\": [\n",
      "    { \"nom\": \"Sushi\", \"pays\": \"Japon\" },\n",
      "    { \"nom\": \"Pizza\", \"pays\": \"Italie\" },\n",
      "    { \"nom\": \"Tacos\", \"pays\": \"Mexique\" },\n",
      "    { \"nom\": \"Couscous\", \"pays\": \"Maroc\" },\n",
      "    { \"nom\": \"Paella\", \"pays\": \"Espagne\" },\n",
      "    { \"nom\": \"Croissant\", \"pays\": \"France\" },\n",
      "    { \"nom\": \"Hamburger\", \"pays\": \"√âtats-Unis\" },\n",
      "    { \"nom\": \"Poutine\", \"pays\": \"Canada\" },\n",
      "    { \"nom\": \"Kimchi\", \"pays\": \"Cor√©e du Sud\" },\n",
      "    { \"nom\": \"Curry\", \"pays\": \"Inde\" }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Initialisation du client OpenAI avec la cl√© API (via variable d'environnement)\n",
    "client = openai.OpenAI(organization=None,\n",
    "                                project=None, \n",
    "                                timeout=60*5, #5 minutes and break after that\n",
    "                                max_retries=2,\n",
    "                                api_key=os.getenv(\"API_KEY\")) # Remplace si besoin\n",
    "\n",
    "def get_famous_dishes_by_country():\n",
    "    # Prompt demand√©\n",
    "    prompt = (\n",
    "        \"Donne-moi une liste des 10 plats les plus connus dans le monde, \"\n",
    "        \"tri√©e par pays, au format JSON. Pour chaque plat, indique le nom du plat et le pays d'origine. \"\n",
    "        \"Utilise ce format : \"\n",
    "        '{ \"plats\": [ { \"nom\": \"Sushi\", \"pays\": \"Japon\" }, ... ] }'\n",
    "    )\n",
    "\n",
    "    # Appel √† l'API avec la m√©thode chat.completions.create\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # ou \"gpt-4\"\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Tu es un assistant culinaire qui fournit des r√©ponses en JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        max_tokens=500,\n",
    "        response_format={\"type\":\"json_object\"} # N√©cessite GPT-4-turbo ou GPT-4o\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    json_result = get_famous_dishes_by_country()\n",
    "    print(json_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function tools \n",
    "\n",
    "Function tools (also called tools or function calling in OpenAI) let you connect a chatbot to external functions in your code.\n",
    "\n",
    "üîß In simple terms:\n",
    "Function tools allow the chatbot to call real functions ‚Äî like searching a database, getting the weather, or running a calculation ‚Äî based on the user's question.\n",
    "\n",
    "üìå Example:\n",
    "If the user says:\n",
    "\n",
    "\"What's the weather in Paris?\"\n",
    "\n",
    "The model can call your get_weather(city) function, get the result, and reply with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Job Offer:\n",
      " **Job Offer: Data Scientist**\n",
      "\n",
      "**Company:** Microsoft  \n",
      "**Location:** Montreal, Quebec, Canada  \n",
      "**Position:** Data Scientist  \n",
      "**Salary:** $120,000 CAD per year  \n",
      "\n",
      "**About the Role:**  \n",
      "Microsoft is seeking a talented and motivated Data Scientist to join our dynamic team in Montreal. In this role, you will leverage cutting-edge technologies and advanced analytical techniques to drive data-driven decision-making and contribute to impactful projects across the organization.\n",
      "\n",
      "**Key Responsibilities:**  \n",
      "- Develop, implement, and maintain machine learning models and algorithms.\n",
      "- Analyze large datasets to extract actionable insights and business value.\n",
      "- Collaborate with cross-functional teams to understand their data needs and provide innovative solutions.\n",
      "- Present findings and recommendations to stakeholders in a clear and concise manner.\n",
      "- Continuously stay updated with the latest industry trends and advancements in data science.\n",
      "\n",
      "**Qualifications:**  \n",
      "- Bachelor‚Äôs or Master‚Äôs degree in Data Science, Computer Science, Statistics, or a related field.\n",
      "- Proven experience in data science, machine learning, or statistical analysis.\n",
      "- Strong proficiency in programming languages such as Python or R.\n",
      "- Familiarity with data visualization tools and techniques.\n",
      "- Excellent problem-solving skills and attention to detail.\n",
      "- Strong communication skills, both written and verbal.\n",
      "\n",
      "**What We Offer:**  \n",
      "- Competitive salary of $120,000 CAD.\n",
      "- Comprehensive benefits package, including health insurance and retirement plans.\n",
      "- Opportunities for professional growth and development within Microsoft.\n",
      "- A collaborative and inclusive work environment.\n",
      "- Access to the latest Microsoft technologies and tools.\n",
      "\n",
      "**How to Apply:**  \n",
      "Interested candidates should submit their resume and a cover letter outlining their relevant experience and how they can contribute to Microsoft‚Äôs success in the field of data science.\n",
      "\n",
      "**Microsoft is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.**\n",
      "\n",
      "**Application Deadline:** [Insert Deadline Date]\n",
      "\n",
      "We look forward to receiving your application!\n",
      "\n",
      "------------------------------\n",
      "üß† Extracted Info:\n",
      "üìå Position      : Data Scientist\n",
      "üèôÔ∏è  City          : Montreal, Quebec, Canada\n",
      "üíº Company Name : Microsoft\n",
      "üí∞ Salary        : $120,000 CAD per year\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "\n",
    "system = \"You're an AI agent capable of analyzing job offers\"\n",
    "prompt = \"Create a simple job offer for a data scientist position in Microsoft in Montreal and salary equal to 120,000$ \"\n",
    "\n",
    "try:\n",
    "    # Cr√©ation de l'offre\n",
    "    client = openai.OpenAI(api_key=os.getenv(\"API_KEY\"))\n",
    "\n",
    "    response1 = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    job_offer = response1.choices[0].message.content\n",
    "\n",
    "    # Extraction des informations\n",
    "    response2 = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": job_offer}\n",
    "        ],\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"extract_job_info\",\n",
    "                    \"description\": \"Extract key information from a job offer: position, city, salary, and company name.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"job_position\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The job title (e.g., Data Scientist).\"\n",
    "                            },\n",
    "                            \"city\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"City where the job is located.\"\n",
    "                            },\n",
    "                            \"salary\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Salary for the job.\"\n",
    "                            },\n",
    "                            \"company_name\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Company offering the position.\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"job_position\", \"city\", \"salary\", \"company_name\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"extract_job_info\"}}\n",
    "    )\n",
    "\n",
    "    print(\"üìÑ Job Offer:\\n\", job_offer)\n",
    "    print(\"\\n------------------------------\")\n",
    "\n",
    "    # Extraire les arguments du tool_call\n",
    "    tool_call = response2.choices[0].message.tool_calls[0]\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "    print(\"üß† Extracted Info:\")\n",
    "    print(f\"üìå Position      : {args['job_position']}\")\n",
    "    print(f\"üèôÔ∏è  City          : {args['city']}\")\n",
    "    print(f\"üíº Company Name : {args['company_name']}\")\n",
    "    print(f\"üí∞ Salary        : {args['salary']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LLM Apps with LangChain\n",
    "\n",
    ">What is LangChain and Why Use It?\n",
    "\n",
    "LangChain is a powerful framework for building applications powered by language models like GPT-4. It helps developers connect LLMs with external data (like PDFs, databases, APIs), manage conversation memory, and build complex workflows using tools and agents.\n",
    "\n",
    ">Why use LangChain?\n",
    "\n",
    "Because it simplifies the process of building smart, context-aware applications with LLMs ‚Äî like chatbots, search engines, or autonomous agents ‚Äî and makes them more scalable, modular, and production-ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM output:\n",
      "-----------------------\n",
      "LangChain is an open-source framework designed for building applications that incorporate language models. It provides tools and components that make it easier to develop sophisticated applications that can leverage natural language processing (NLP) capabilities. LangChain is particularly useful for creating chatbots, AI assistants, and other applications that require the integration of language models with external data sources and APIs.\n",
      "\n",
      "Key features of LangChain include:\n",
      "\n",
      "1. **Modular Design**: LangChain allows developers to compose different components like models, prompt templates, and chains of operations to build complex applications.\n",
      "\n",
      "2. **Chain Abstractions**: Developers can create chains of operations that combine multiple actions, such as prompting a language model, retrieving information from a database, or making API calls.\n",
      "\n",
      "3. **Memory**: It provides options to integrate memory into applications, allowing them to remember past interactions and context to improve user experience.\n",
      "\n",
      "4. **Integration with Data Sources**: LangChain supports connections to various data sources, which means it can pull in information from databases, APIs, or other external systems to enhance its responses.\n",
      "\n",
      "5. **Prompt Management**: It offers utilities for managing prompts, which are essential in guiding the responses of language models.\n",
      "\n",
      "LangChain is aimed at developers looking to build custom applications using advanced language models, and it facilitates smoother integration of those models into practical, real-world applications.\n"
     ]
    }
   ],
   "source": [
    "# pip install langchain\n",
    "# pip install langchain_openai\n",
    "\n",
    "#Import LangChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "#Build an llm\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=os.getenv(\"API_KEY\"),\n",
    "    base_url=None, organization=None, seed=None)\n",
    "\n",
    "prompt=\"What is LangChain ?\"\n",
    "response = llm.invoke(prompt)\n",
    "print(\"\\nLLM output:\")\n",
    "print(f\"-----------------------\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting Hugging Face models\n",
    "\n",
    ">Prompting Hugging Face Models with LangChain: What & Why\n",
    "\n",
    "LangChain supports integration with Hugging Face models, allowing you to use open-source LLMs (like BLOOM, Falcon, or Mistral) in your applications.\n",
    "\n",
    ">What does it mean to prompt Hugging Face models in LangChain?\n",
    "\n",
    "It means you can send inputs (prompts) to Hugging Face-hosted models via LangChain and process the outputs as part of a chain, agent, or tool-based system.\n",
    "\n",
    ">Why use LangChain with Hugging Face models?\n",
    "\n",
    "Because it gives you the flexibility of open-source models while still benefiting from LangChain‚Äôs structure ‚Äî such as chaining logic, memory management, and tool use ‚Äî without relying solely on OpenAI or closed APIs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#pip install huggingface_hub\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#pip install langchain_huggingface\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#pip install transformers\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#pip install torch torchvision\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFacePipeline \n\u001b[0;32m----> 8\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFacePipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_model_id\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtiiuae/falcon-rw-1b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#Choose a model llm from huggingface hub\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_new_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_huggingface/llms/huggingface_pipeline.py:263\u001b[0m, in \u001b[0;36mHuggingFacePipeline.from_model_id\u001b[0;34m(cls, model_id, task, backend, device, device_map, model_kwargs, pipeline_kwargs, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m     _model_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    260\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _model_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m     }\n\u001b[1;32m    262\u001b[0m _pipeline_kwargs \u001b[38;5;241m=\u001b[39m pipeline_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m--> 263\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mhf_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_pipeline_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pipeline\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m VALID_TASKS:\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot invalid task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrently only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mVALID_TASKS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/transformers/pipelines/__init__.py:1180\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1178\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m processor\n\u001b[0;32m-> 1180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/transformers/pipelines/text_generation.py:114\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_model_type(\n\u001b[1;32m    116\u001b[0m         TF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_params:\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;66;03m# This is very specific. The logic is quite complex and needs to be done\u001b[39;00m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;66;03m# as a \"default\".\u001b[39;00m\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;66;03m# It also defines both some preprocess_kwargs and generate_kwargs\u001b[39;00m\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;66;03m# which is why we cannot put them in their respective methods.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/transformers/pipelines/base.py:1016\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# We shouldn't call `model.to()` for models loaded with accelerate as well as the case that model is already on device\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m hf_device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m ):\n\u001b[0;32m-> 1016\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;66;03m# If it's a generation pipeline and the model can generate:\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;66;03m# 1 - create a local generation config. This is done to avoid side-effects on the model as we apply local\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;66;03m# tweaks to the generation config.\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# 2 - load the assistant model if it is passed.\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipeline_calls_generate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcan_generate():\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/transformers/modeling_utils.py:3851\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3846\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3847\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3848\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3849\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3850\u001b[0m         )\n\u001b[0;32m-> 3851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 915 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#pip install huggingface_hub\n",
    "#pip install langchain_huggingface\n",
    "#pip install transformers\n",
    "#pip install torch torchvision\n",
    "\n",
    "from langchain_huggingface import HuggingFacePipeline \n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id= \"tiiuae/falcon-rw-1b\", #Choose a model llm from huggingface hub\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\":100}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a powerful framework designed for developing applications using large language models (LLMs). Here are three reasons to consider using LangChain for LLM application development:\n",
      "\n",
      "1. **Modularity and Composability**: LangChain offers a modular architecture that allows developers to easily compose different components, such as prompt templates, memory management, and chains of operations. This flexibility enables developers to build complex applications by combining simple building blocks, making it easier to iterate on designs and implement varied functionalities.\n",
      "\n",
      "2. **Integration with External APIs and Tools**: LangChain can seamlessly integrate with various external APIs, databases, and tools, allowing developers to enhance the capabilities of LLMs beyond text generation. This includes integrating with search engines, knowledge bases, and even other machine learning models, enabling the creation of more interactive and context-aware applications.\n",
      "\n",
      "3. **Built-In Support for Prompt Engineering and Memory**: LangChain includes features that simplify prompt engineering and memory management, which are critical for effective LLM interactions. It provides utilities for template creation, dynamic prompts, and managing conversation history, ensuring that the models can maintain context and provide relevant responses over extended interactions.\n",
      "\n",
      "These features make LangChain an attractive option for developers looking to harness the power of LLMs in a structured and efficient manner.\n"
     ]
    }
   ],
   "source": [
    "#Define the LLM:\n",
    "llm = ChatOpenAI(model='gpt-4o-mini',\n",
    "                 api_key=os.getenv(\"API_KEY\")\n",
    "                 )\n",
    "#Predict the words following the text in question:\n",
    "prompt = \"Three reasons for using LangChain for LLM application development\"\n",
    "#Get response:\n",
    "response = llm.invoke(prompt)\n",
    "#Print response\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  /n What is the capital of France ?\n",
      "\n",
      "The capital of France, France, is a province in the French province of France in the southern part of the French province of France. The land is divided into two regions: the provinces of France and the provinces of France.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Import the class for defining Hugging Face pipelines:\n",
    "from langchain_openai import ChatOpenAI \n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "#Define the LLM from the Hugging Face model ID:\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"crumb/nano-mistral\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 50}\n",
    ")\n",
    "#Define the prompt:\n",
    "prompt = \"What is the capital of France ?\"\n",
    "print(f\"Output:  /n {llm.invoke(prompt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm unable to provide real-time weather updates. To check the current weather in Montreal, I recommend using a reliable weather website or app. You can find up-to-date information on temperature, conditions, and forecasts there.\n"
     ]
    }
   ],
   "source": [
    "#Asking for real-time data such as weather:\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = \"What is the weather in Montreal ?\"\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini',\n",
    "                 api_key=os.getenv(\"API_KEY\"))\n",
    "print(llm.invoke(prompt).content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a versatile framework designed to simplify the development of applications using Large Language Models (LLMs). Here are three key reasons for using LangChain in LLM application development:\n",
      "\n",
      "1. **Modularity and Extensibility**: LangChain is built with a modular architecture, allowing developers to easily integrate various components such as models, data connectors, and workflows. This modularity facilitates the customization and extension of applications, enabling developers to tweak individual components or swap them out as needed without rewriting the entire application.\n",
      "\n",
      "2. **Integrated Pipelines**: LangChain provides built-in support for creating end-to-end pipelines for LLM applications. It allows developers to seamlessly chain together different processing steps, such as data retrieval, pre-processing, and post-processing, while leveraging LLMs. This can streamline the development process by automating tasks and ensuring efficient data flow, making it easier to build complex applications that require multiple interactions with the model.\n",
      "\n",
      "3. **Rich Ecosystem and Community**: LangChain has fostered a thriving ecosystem and growing community of developers and contributors. This means that users can benefit from a wide array of pre-built modules, templates, and community-driven resources, which can significantly reduce development time and effort. Additionally, the community's support and ongoing improvements contribute to the framework's robustness and adaptability to changing needs in LLM application development.\n",
      "\n",
      "Overall, LangChain simplifies the complexities of working with LLMs, enhances productivity, and fosters innovation through its modular design and strong community support.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face is an exciting way to show your support for the NHS and the NHS that matters.\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "# Define the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"API_KEY\"))\n",
    "prompt = 'Three reasons for using LangChain for LLM application development.'\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)\n",
    "\n",
    "# Import the class for defining Hugging Face pipelines\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "# Define the LLM from the Hugging Face model ID\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"crumb/nano-mistral\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 20}\n",
    ")\n",
    "\n",
    "prompt = \"Hugging Face is\"\n",
    "response = llm.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    üß© What is LangChain?\n",
    "LangChain is like a magic tool that helps you talk to AI (like ChatGPT) and build smart apps. It helps you give instructions to AI in a smart and reusable way.\n",
    "\n",
    "    üìù What is a PromptTemplate in LangChain?\n",
    "Let‚Äôs say you‚Äôre building an app that always asks the AI:\n",
    "\n",
    "‚ÄúWhat is a fun fact about ___?‚Äù\n",
    "\n",
    "But instead of writing the whole question every time, you want a shortcut.\n",
    "\n",
    "In LangChain, you make a PromptTemplate like this:\n",
    "\n",
    "- from langchain.prompts import PromptTemplate\n",
    "- template = PromptTemplate.from_template(\"What is a fun fact about {topic}?\")\n",
    "The {topic} part is like a blank you can fill in later.\n",
    "\n",
    "    üéâ Why is it cool?\n",
    "    \n",
    "You only write the question once.\n",
    "You can reuse it again and again by changing the topic.\n",
    "It makes your app or chatbot faster and smarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x3bc23c0d0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x3bc23c250> root_client=<openai.OpenAI object at 0x136aee7b0> root_async_client=<openai.AsyncOpenAI object at 0x136aef230> model_name='gpt-4o-mini' model_kwargs={} openai_api_key=SecretStr('**********')\n",
      "LangChain simplifies LLM (Large Language Model) application development by providing a framework that streamlines the process of building applications around language models. Here are some key ways it accomplishes this:\n",
      "\n",
      "1. **Modular Components**: LangChain offers various modular components that developers can easily plug into their applications. These include tools for prompt management, chat formats, and ways to handle inputs and outputs.\n",
      "\n",
      "2. **Prompt Engineering**: It provides utilities for creating and managing prompts, allowing developers to fine-tune their interactions with LLMs without needing to start from scratch each time.\n",
      "\n",
      "3. **Chains**: LangChain allows developers to create chains of operations, where the output of one step can be the input to another. This is particularly useful for building more complex applications that require multiple processing steps.\n",
      "\n",
      "4. **Integration with External Data Sources**: LangChain can integrate with various data sources and APIs, making it easier to connect an LLM with real-time data and enhance its capabilities with up-to-date information.\n",
      "\n",
      "5. **State Management**: It handles state management for conversational applications, keeping track of the dialogue context across interactions, which is crucial for building chatbots or virtual assistants.\n",
      "\n",
      "6. **Testing and Evaluation Tools**: LangChain includes built-in tools for testing and evaluating LLM applications, enabling developers to assess the model's performance and make necessary adjustments more efficiently.\n",
      "\n",
      "7. **Community and Ecosystem**: Being a part of a growing community, developers can benefit from shared resources, examples, and support, which accelerates the development process.\n",
      "\n",
      "8. **Scalability**: LangChain is designed to be scalable, allowing developers to build small prototypes and gradually expand them into more comprehensive applications as needed.\n",
      "\n",
      "By providing these features and utilities, LangChain reduces the complexity usually associated with LLM application development, enabling developers to focus more on functionality and user experience rather than the underlying technical challenges.\n"
     ]
    }
   ],
   "source": [
    "#Import packages from LangChain:\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "#Create a prompt template specifying the AI Behaviour:\n",
    "template = \"You are an artificial intelligence assistant, answer the question. {question}\"\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=template\n",
    "    )\n",
    "#Create the LLM:\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"API_KEY\"))\n",
    "print(llm)\n",
    "\n",
    "#Create a chain to integrate the prompt template and LLM using LCEL (LangChain Expression Language):\n",
    "llm_chain = prompt_template | llm \n",
    "llm_chain\n",
    "\n",
    "question = \"How does LangChain make LLM application development easier ?\"\n",
    "print(llm_chain.invoke(question).content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatModels\n",
    "Chat roles : **system**, **human** and **ai**\n",
    "\n",
    "- system\tüé¨ The director\tSets the rules and tone (e.g., \"You are a helpful assistant\")\n",
    "- human\tüë¶ The person talking\tAsks questions or gives instructions\n",
    "- AI (or assistant)\tü§ñ The actor / chatbot\tReplies based on what the human said and the system rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, got list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatOpenAI(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m, api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#Create the prompte template:\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m prompt_template \u001b[38;5;241m=\u001b[39m \u001b[43mChatPromptTemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#Behaviour's artificial intelligent agent (system):\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a geography expert that returns the colors present in a country flag.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#User's question (user):\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhumain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFrance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#AI's answer (assistant/ai):\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mai\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBlue,White,Red\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#User's question (user):\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhumain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{country}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_core/prompts/chat.py:1088\u001b[0m, in \u001b[0;36mChatPromptTemplate.from_template\u001b[0;34m(cls, template, **kwargs)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_template\u001b[39m(\u001b[38;5;28mcls\u001b[39m, template: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatPromptTemplate:\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a chat prompt template from a template string.\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m \n\u001b[1;32m   1078\u001b[0m \u001b[38;5;124;03m    Creates a chat template consisting of a single message assumed to be from\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;124;03m        A new instance of this class.\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m     prompt_template \u001b[38;5;241m=\u001b[39m \u001b[43mPromptTemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1089\u001b[0m     message \u001b[38;5;241m=\u001b[39m HumanMessagePromptTemplate(prompt\u001b[38;5;241m=\u001b[39mprompt_template)\n\u001b[1;32m   1090\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_messages([message])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_core/prompts/prompt.py:289\u001b[0m, in \u001b[0;36mPromptTemplate.from_template\u001b[0;34m(cls, template, template_format, partial_variables, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_template\u001b[39m(\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    258\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptTemplate:\n\u001b[1;32m    259\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a prompt template from a template.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03m    *Security warning*:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m        The prompt template loaded from the template.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m     input_variables \u001b[38;5;241m=\u001b[39m \u001b[43mget_template_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m     _partial_variables \u001b[38;5;241m=\u001b[39m partial_variables \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _partial_variables:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_core/prompts/string.py:255\u001b[0m, in \u001b[0;36mget_template_variables\u001b[0;34m(template, template_format)\u001b[0m\n\u001b[1;32m    252\u001b[0m     input_variables \u001b[38;5;241m=\u001b[39m _get_jinja2_variables_from_template(template)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m template_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf-string\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    254\u001b[0m     input_variables \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 255\u001b[0m         v \u001b[38;5;28;01mfor\u001b[39;00m _, v, _, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[43mFormatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     }\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m template_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmustache\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    258\u001b[0m     input_variables \u001b[38;5;241m=\u001b[39m mustache_template_vars(template)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/string.py:288\u001b[0m, in \u001b[0;36mFormatter.parse\u001b[0;34m(self, format_string)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string):\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_string\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformatter_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, got list"
     ]
    }
   ],
   "source": [
    "#Import packages:\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "#Create the LLM:\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"API_KEY\"))\n",
    "\n",
    "#Create the prompte template:\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    [\n",
    "        #Behaviour's artificial intelligent agent (system):\n",
    "        (\"system\",\"You are a geography expert that returns the colors present in a country flag.\"),\n",
    "        #User's question (user):\n",
    "        (\"humain\",\"France\"),\n",
    "        #AI's answer (assistant/ai):\n",
    "        (\"ai\",\"Blue,White,Red\"),\n",
    "        #User's question (user):\n",
    "        (\"humain\",\"{country}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm_chain = prompt_template | llm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
