{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing AI System\n",
    "\n",
    "### Chatbot\n",
    "\n",
    "A **chatbot** is a computer program that simulates human conversation, usually through text or voice. With **OpenAI**, a chatbot uses powerful language models like **GPT-4** to understand questions and generate smart, human-like answers.\n",
    "\n",
    "In simple terms:\n",
    "\n",
    "> A chatbot with OpenAI can chat, answer questions, summarize info, or help with tasks — almost like talking to a helpful assistant.\n",
    "\n",
    "It works by sending messages to the model (input) and receiving a response (output), often using the `chat.completions.create()` function from the OpenAI API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexBot is ready. Tape 'exit' to quit the conversation\n",
      "AlexBot : Hello! How can I assist you today?\n",
      "--------------------------------------------------\n",
      "Thanks for choosing AlexBot. See you soon !\n"
     ]
    }
   ],
   "source": [
    "#This version of chatbot can only be running through the console,\n",
    "#Check code app.py a version of this chatbot deployed on Streamlit Platform for a better experience\n",
    "#To run the chatbot --> Terminal : streamlit run app.py\n",
    "\n",
    "import os \n",
    "import openai\n",
    "from time import sleep\n",
    "from tqdm import tgrange, tqdm_notebook\n",
    "\n",
    "#Bloc\n",
    "MESSAGES = [{\"role\":\"system\", \"content\": \"You're a Chatbot, your name is AlexBot you've been created by Alexandre Ohayon, you can answer any customer's questions by summarizing your answer.\"}]\n",
    "#Dialog Box ask your question to the ChatBot:\n",
    "\n",
    "try:\n",
    "    print(\"AlexBot is ready. Tape 'exit' to quit the conversation\")\n",
    "\n",
    "    #Start a session:\n",
    "    while True:\n",
    "        #Enter your question:\n",
    "        USER_PROMPT = input(\"Enter your message here: \") #Use the dialog box\n",
    "        #Tape exist in lower character to quit the conversation with the Bot.\n",
    "        if USER_PROMPT.lower() == \"exit\": \n",
    "            print(\"Thanks for choosing AlexBot. See you soon !\") #Message from system\n",
    "            break #Session closed\n",
    "            \n",
    "        #If a conversation exists then add user question to the Bloc in line 7:\n",
    "        MESSAGES.append({\"role\":\"user\", \"content\": USER_PROMPT}) #Add the question into the list:\n",
    "        user_question = {\"role\":\"user\", \"content\": USER_PROMPT}\n",
    "        #print(f\"User_question : {USER_PROMPT}\")\n",
    "\n",
    "                \n",
    "        #Pass MESSAGES through the LLM to get response from the AI Agent:\n",
    "        client = openai.OpenAI(organization=None,\n",
    "                                project=None, \n",
    "                                timeout=60*5, #5 minutes and break after that\n",
    "                                max_retries=2,\n",
    "                                api_key=os.getenv(\"API_KEY\"))\n",
    "            \n",
    "        #Response from the Intelligent Agent:\n",
    "        # Add the appropriate parameters to the decorator\n",
    "        response = client.chat.completions.create(\n",
    "            model='gpt-4o-mini',\n",
    "            messages=MESSAGES,\n",
    "            max_tokens=300,\n",
    "            temperature=0,\n",
    "            top_p=0.9,\n",
    "            stop=[\"\\n\"]\n",
    "        )\n",
    "\n",
    "        assistant_dict = {\"role\":\"assistant\", \"content\": response.choices[0].message.content}\n",
    "        print(f\"AlexBot : {assistant_dict[\"content\"]}\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "        #Add the question into the list:\n",
    "        MESSAGES.append(assistant_dict)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error :\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get response in Json Format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"plats\": [\n",
      "    { \"nom\": \"Sushi\", \"pays\": \"Japon\" },\n",
      "    { \"nom\": \"Pizza\", \"pays\": \"Italie\" },\n",
      "    { \"nom\": \"Tacos\", \"pays\": \"Mexique\" },\n",
      "    { \"nom\": \"Couscous\", \"pays\": \"Maroc\" },\n",
      "    { \"nom\": \"Paella\", \"pays\": \"Espagne\" },\n",
      "    { \"nom\": \"Croissant\", \"pays\": \"France\" },\n",
      "    { \"nom\": \"Hamburger\", \"pays\": \"États-Unis\" },\n",
      "    { \"nom\": \"Poutine\", \"pays\": \"Canada\" },\n",
      "    { \"nom\": \"Kimchi\", \"pays\": \"Corée du Sud\" },\n",
      "    { \"nom\": \"Curry\", \"pays\": \"Inde\" }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Initialisation du client OpenAI avec la clé API (via variable d'environnement)\n",
    "client = openai.OpenAI(organization=None,\n",
    "                                project=None, \n",
    "                                timeout=60*5, #5 minutes and break after that\n",
    "                                max_retries=2,\n",
    "                                api_key=os.getenv(\"API_KEY\")) # Remplace si besoin\n",
    "\n",
    "def get_famous_dishes_by_country():\n",
    "    # Prompt demandé\n",
    "    prompt = (\n",
    "        \"Donne-moi une liste des 10 plats les plus connus dans le monde, \"\n",
    "        \"triée par pays, au format JSON. Pour chaque plat, indique le nom du plat et le pays d'origine. \"\n",
    "        \"Utilise ce format : \"\n",
    "        '{ \"plats\": [ { \"nom\": \"Sushi\", \"pays\": \"Japon\" }, ... ] }'\n",
    "    )\n",
    "\n",
    "    # Appel à l'API avec la méthode chat.completions.create\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # ou \"gpt-4\"\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Tu es un assistant culinaire qui fournit des réponses en JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        max_tokens=500,\n",
    "        response_format={\"type\":\"json_object\"} # Nécessite GPT-4-turbo ou GPT-4o\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    json_result = get_famous_dishes_by_country()\n",
    "    print(json_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function tools \n",
    "\n",
    "Function tools (also called tools or function calling in OpenAI) let you connect a chatbot to external functions in your code.\n",
    "\n",
    "🔧 In simple terms:\n",
    "Function tools allow the chatbot to call real functions — like searching a database, getting the weather, or running a calculation — based on the user's question.\n",
    "\n",
    "📌 Example:\n",
    "If the user says:\n",
    "\n",
    "\"What's the weather in Paris?\"\n",
    "\n",
    "The model can call your get_weather(city) function, get the result, and reply with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Job Offer:\n",
      " **Job Offer: Data Scientist**\n",
      "\n",
      "**Company:** Microsoft  \n",
      "**Location:** Montreal, Quebec, Canada  \n",
      "**Position:** Data Scientist  \n",
      "**Salary:** $120,000 CAD per year  \n",
      "\n",
      "**About the Role:**  \n",
      "Microsoft is seeking a talented and motivated Data Scientist to join our dynamic team in Montreal. In this role, you will leverage cutting-edge technologies and advanced analytical techniques to drive data-driven decision-making and contribute to impactful projects across the organization.\n",
      "\n",
      "**Key Responsibilities:**  \n",
      "- Develop, implement, and maintain machine learning models and algorithms.\n",
      "- Analyze large datasets to extract actionable insights and business value.\n",
      "- Collaborate with cross-functional teams to understand their data needs and provide innovative solutions.\n",
      "- Present findings and recommendations to stakeholders in a clear and concise manner.\n",
      "- Continuously stay updated with the latest industry trends and advancements in data science.\n",
      "\n",
      "**Qualifications:**  \n",
      "- Bachelor’s or Master’s degree in Data Science, Computer Science, Statistics, or a related field.\n",
      "- Proven experience in data science, machine learning, or statistical analysis.\n",
      "- Strong proficiency in programming languages such as Python or R.\n",
      "- Familiarity with data visualization tools and techniques.\n",
      "- Excellent problem-solving skills and attention to detail.\n",
      "- Strong communication skills, both written and verbal.\n",
      "\n",
      "**What We Offer:**  \n",
      "- Competitive salary of $120,000 CAD.\n",
      "- Comprehensive benefits package, including health insurance and retirement plans.\n",
      "- Opportunities for professional growth and development within Microsoft.\n",
      "- A collaborative and inclusive work environment.\n",
      "- Access to the latest Microsoft technologies and tools.\n",
      "\n",
      "**How to Apply:**  \n",
      "Interested candidates should submit their resume and a cover letter outlining their relevant experience and how they can contribute to Microsoft’s success in the field of data science.\n",
      "\n",
      "**Microsoft is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.**\n",
      "\n",
      "**Application Deadline:** [Insert Deadline Date]\n",
      "\n",
      "We look forward to receiving your application!\n",
      "\n",
      "------------------------------\n",
      "🧠 Extracted Info:\n",
      "📌 Position      : Data Scientist\n",
      "🏙️  City          : Montreal, Quebec, Canada\n",
      "💼 Company Name : Microsoft\n",
      "💰 Salary        : $120,000 CAD per year\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "\n",
    "system = \"You're an AI agent capable of analyzing job offers\"\n",
    "prompt = \"Create a simple job offer for a data scientist position in Microsoft in Montreal and salary equal to 120,000$ \"\n",
    "\n",
    "try:\n",
    "    # Création de l'offre\n",
    "    client = openai.OpenAI(api_key=os.getenv(\"API_KEY\"))\n",
    "\n",
    "    response1 = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    job_offer = response1.choices[0].message.content\n",
    "\n",
    "    # Extraction des informations\n",
    "    response2 = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": job_offer}\n",
    "        ],\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"extract_job_info\",\n",
    "                    \"description\": \"Extract key information from a job offer: position, city, salary, and company name.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"job_position\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The job title (e.g., Data Scientist).\"\n",
    "                            },\n",
    "                            \"city\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"City where the job is located.\"\n",
    "                            },\n",
    "                            \"salary\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Salary for the job.\"\n",
    "                            },\n",
    "                            \"company_name\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Company offering the position.\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"job_position\", \"city\", \"salary\", \"company_name\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"extract_job_info\"}}\n",
    "    )\n",
    "\n",
    "    print(\"📄 Job Offer:\\n\", job_offer)\n",
    "    print(\"\\n------------------------------\")\n",
    "\n",
    "    # Extraire les arguments du tool_call\n",
    "    tool_call = response2.choices[0].message.tool_calls[0]\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "    print(\"🧠 Extracted Info:\")\n",
    "    print(f\"📌 Position      : {args['job_position']}\")\n",
    "    print(f\"🏙️  City          : {args['city']}\")\n",
    "    print(f\"💼 Company Name : {args['company_name']}\")\n",
    "    print(f\"💰 Salary        : {args['salary']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"❌ Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LLM Apps with LangChain\n",
    "\n",
    ">What is LangChain and Why Use It?\n",
    "\n",
    "LangChain is a powerful framework for building applications powered by language models like GPT-4. It helps developers connect LLMs with external data (like PDFs, databases, APIs), manage conversation memory, and build complex workflows using tools and agents.\n",
    "\n",
    ">Why use LangChain?\n",
    "\n",
    "Because it simplifies the process of building smart, context-aware applications with LLMs — like chatbots, search engines, or autonomous agents — and makes them more scalable, modular, and production-ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM output:\n",
      "-----------------------\n",
      "LangChain is an open-source framework designed for building applications that incorporate language models. It provides tools and components that make it easier to develop sophisticated applications that can leverage natural language processing (NLP) capabilities. LangChain is particularly useful for creating chatbots, AI assistants, and other applications that require the integration of language models with external data sources and APIs.\n",
      "\n",
      "Key features of LangChain include:\n",
      "\n",
      "1. **Modular Design**: LangChain allows developers to compose different components like models, prompt templates, and chains of operations to build complex applications.\n",
      "\n",
      "2. **Chain Abstractions**: Developers can create chains of operations that combine multiple actions, such as prompting a language model, retrieving information from a database, or making API calls.\n",
      "\n",
      "3. **Memory**: It provides options to integrate memory into applications, allowing them to remember past interactions and context to improve user experience.\n",
      "\n",
      "4. **Integration with Data Sources**: LangChain supports connections to various data sources, which means it can pull in information from databases, APIs, or other external systems to enhance its responses.\n",
      "\n",
      "5. **Prompt Management**: It offers utilities for managing prompts, which are essential in guiding the responses of language models.\n",
      "\n",
      "LangChain is aimed at developers looking to build custom applications using advanced language models, and it facilitates smoother integration of those models into practical, real-world applications.\n"
     ]
    }
   ],
   "source": [
    "# pip install langchain\n",
    "# pip install langchain_openai\n",
    "\n",
    "#Import LangChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "#Build an llm\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=os.getenv(\"API_KEY\"),\n",
    "    base_url=None, organization=None, seed=None)\n",
    "\n",
    "prompt=\"What is LangChain ?\"\n",
    "response = llm.invoke(prompt)\n",
    "print(\"\\nLLM output:\")\n",
    "print(f\"-----------------------\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting Hugging Face models\n",
    "\n",
    ">Prompting Hugging Face Models with LangChain: What & Why\n",
    "\n",
    "LangChain supports integration with Hugging Face models, allowing you to use open-source LLMs (like BLOOM, Falcon, or Mistral) in your applications.\n",
    "\n",
    ">What does it mean to prompt Hugging Face models in LangChain?\n",
    "\n",
    "It means you can send inputs (prompts) to Hugging Face-hosted models via LangChain and process the outputs as part of a chain, agent, or tool-based system.\n",
    "\n",
    ">Why use LangChain with Hugging Face models?\n",
    "\n",
    "Because it gives you the flexibility of open-source models while still benefiting from LangChain’s structure — such as chaining logic, memory management, and tool use — without relying solely on OpenAI or closed APIs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#pip install huggingface_hub\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#pip install langchain_huggingface\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#pip install transformers\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#pip install torch torchvision\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFacePipeline \n\u001b[0;32m----> 8\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFacePipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_model_id\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtiiuae/falcon-rw-1b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#Choose a model llm from huggingface hub\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_new_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_huggingface/llms/huggingface_pipeline.py:263\u001b[0m, in \u001b[0;36mHuggingFacePipeline.from_model_id\u001b[0;34m(cls, model_id, task, backend, device, device_map, model_kwargs, pipeline_kwargs, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m     _model_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    260\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _model_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m     }\n\u001b[1;32m    262\u001b[0m _pipeline_kwargs \u001b[38;5;241m=\u001b[39m pipeline_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m--> 263\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mhf_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_pipeline_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pipeline\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m VALID_TASKS:\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot invalid task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrently only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mVALID_TASKS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/transformers/pipelines/__init__.py:1180\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1178\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m processor\n\u001b[0;32m-> 1180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/transformers/pipelines/text_generation.py:114\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_model_type(\n\u001b[1;32m    116\u001b[0m         TF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_params:\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;66;03m# This is very specific. The logic is quite complex and needs to be done\u001b[39;00m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;66;03m# as a \"default\".\u001b[39;00m\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;66;03m# It also defines both some preprocess_kwargs and generate_kwargs\u001b[39;00m\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;66;03m# which is why we cannot put them in their respective methods.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/transformers/pipelines/base.py:1016\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# We shouldn't call `model.to()` for models loaded with accelerate as well as the case that model is already on device\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m hf_device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m ):\n\u001b[0;32m-> 1016\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;66;03m# If it's a generation pipeline and the model can generate:\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;66;03m# 1 - create a local generation config. This is done to avoid side-effects on the model as we apply local\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;66;03m# tweaks to the generation config.\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# 2 - load the assistant model if it is passed.\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipeline_calls_generate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcan_generate():\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/transformers/modeling_utils.py:3851\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3846\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3847\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3848\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3849\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3850\u001b[0m         )\n\u001b[0;32m-> 3851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 915 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#pip install huggingface_hub\n",
    "#pip install langchain_huggingface\n",
    "#pip install transformers\n",
    "#pip install torch torchvision\n",
    "\n",
    "from langchain_huggingface import HuggingFacePipeline \n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id= \"tiiuae/falcon-rw-1b\", #Choose a model llm from huggingface hub\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\":100}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a powerful framework designed for developing applications using large language models (LLMs). Here are three reasons to consider using LangChain for LLM application development:\n",
      "\n",
      "1. **Modularity and Composability**: LangChain offers a modular architecture that allows developers to easily compose different components, such as prompt templates, memory management, and chains of operations. This flexibility enables developers to build complex applications by combining simple building blocks, making it easier to iterate on designs and implement varied functionalities.\n",
      "\n",
      "2. **Integration with External APIs and Tools**: LangChain can seamlessly integrate with various external APIs, databases, and tools, allowing developers to enhance the capabilities of LLMs beyond text generation. This includes integrating with search engines, knowledge bases, and even other machine learning models, enabling the creation of more interactive and context-aware applications.\n",
      "\n",
      "3. **Built-In Support for Prompt Engineering and Memory**: LangChain includes features that simplify prompt engineering and memory management, which are critical for effective LLM interactions. It provides utilities for template creation, dynamic prompts, and managing conversation history, ensuring that the models can maintain context and provide relevant responses over extended interactions.\n",
      "\n",
      "These features make LangChain an attractive option for developers looking to harness the power of LLMs in a structured and efficient manner.\n"
     ]
    }
   ],
   "source": [
    "#Define the LLM:\n",
    "llm = ChatOpenAI(model='gpt-4o-mini',\n",
    "                 api_key=os.getenv(\"API_KEY\")\n",
    "                 )\n",
    "#Predict the words following the text in question:\n",
    "prompt = \"Three reasons for using LangChain for LLM application development\"\n",
    "#Get response:\n",
    "response = llm.invoke(prompt)\n",
    "#Print response\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  /n What is the capital of France ?\n",
      "\n",
      "The capital of France, France, is a province in the French province of France in the southern part of the French province of France. The land is divided into two regions: the provinces of France and the provinces of France.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Import the class for defining Hugging Face pipelines:\n",
    "from langchain_openai import ChatOpenAI \n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "#Define the LLM from the Hugging Face model ID:\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"crumb/nano-mistral\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 50}\n",
    ")\n",
    "#Define the prompt:\n",
    "prompt = \"What is the capital of France ?\"\n",
    "print(f\"Output:  /n {llm.invoke(prompt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm unable to provide real-time weather updates. To check the current weather in Montreal, I recommend using a reliable weather website or app. You can find up-to-date information on temperature, conditions, and forecasts there.\n"
     ]
    }
   ],
   "source": [
    "#Asking for real-time data such as weather:\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = \"What is the weather in Montreal ?\"\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini',\n",
    "                 api_key=os.getenv(\"API_KEY\"))\n",
    "print(llm.invoke(prompt).content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a versatile framework designed to simplify the development of applications using Large Language Models (LLMs). Here are three key reasons for using LangChain in LLM application development:\n",
      "\n",
      "1. **Modularity and Extensibility**: LangChain is built with a modular architecture, allowing developers to easily integrate various components such as models, data connectors, and workflows. This modularity facilitates the customization and extension of applications, enabling developers to tweak individual components or swap them out as needed without rewriting the entire application.\n",
      "\n",
      "2. **Integrated Pipelines**: LangChain provides built-in support for creating end-to-end pipelines for LLM applications. It allows developers to seamlessly chain together different processing steps, such as data retrieval, pre-processing, and post-processing, while leveraging LLMs. This can streamline the development process by automating tasks and ensuring efficient data flow, making it easier to build complex applications that require multiple interactions with the model.\n",
      "\n",
      "3. **Rich Ecosystem and Community**: LangChain has fostered a thriving ecosystem and growing community of developers and contributors. This means that users can benefit from a wide array of pre-built modules, templates, and community-driven resources, which can significantly reduce development time and effort. Additionally, the community's support and ongoing improvements contribute to the framework's robustness and adaptability to changing needs in LLM application development.\n",
      "\n",
      "Overall, LangChain simplifies the complexities of working with LLMs, enhances productivity, and fosters innovation through its modular design and strong community support.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face is an exciting way to show your support for the NHS and the NHS that matters.\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "# Define the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"API_KEY\"))\n",
    "prompt = 'Three reasons for using LangChain for LLM application development.'\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)\n",
    "\n",
    "# Import the class for defining Hugging Face pipelines\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "# Define the LLM from the Hugging Face model ID\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"crumb/nano-mistral\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 20}\n",
    ")\n",
    "\n",
    "prompt = \"Hugging Face is\"\n",
    "response = llm.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    🧩 What is LangChain?\n",
    "LangChain is like a magic tool that helps you talk to AI (like ChatGPT) and build smart apps. It helps you give instructions to AI in a smart and reusable way.\n",
    "\n",
    "    📝 What is a PromptTemplate in LangChain?\n",
    "Let’s say you’re building an app that always asks the AI:\n",
    "\n",
    "“What is a fun fact about ___?”\n",
    "\n",
    "But instead of writing the whole question every time, you want a shortcut.\n",
    "\n",
    "In LangChain, you make a PromptTemplate like this:\n",
    "\n",
    "- from langchain.prompts import PromptTemplate\n",
    "- template = PromptTemplate.from_template(\"What is a fun fact about {topic}?\")\n",
    "The {topic} part is like a blank you can fill in later.\n",
    "\n",
    "    🎉 Why is it cool?\n",
    "    \n",
    "You only write the question once.\n",
    "You can reuse it again and again by changing the topic.\n",
    "It makes your app or chatbot faster and smarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x3bc23c0d0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x3bc23c250> root_client=<openai.OpenAI object at 0x136aee7b0> root_async_client=<openai.AsyncOpenAI object at 0x136aef230> model_name='gpt-4o-mini' model_kwargs={} openai_api_key=SecretStr('**********')\n",
      "LangChain simplifies LLM (Large Language Model) application development by providing a framework that streamlines the process of building applications around language models. Here are some key ways it accomplishes this:\n",
      "\n",
      "1. **Modular Components**: LangChain offers various modular components that developers can easily plug into their applications. These include tools for prompt management, chat formats, and ways to handle inputs and outputs.\n",
      "\n",
      "2. **Prompt Engineering**: It provides utilities for creating and managing prompts, allowing developers to fine-tune their interactions with LLMs without needing to start from scratch each time.\n",
      "\n",
      "3. **Chains**: LangChain allows developers to create chains of operations, where the output of one step can be the input to another. This is particularly useful for building more complex applications that require multiple processing steps.\n",
      "\n",
      "4. **Integration with External Data Sources**: LangChain can integrate with various data sources and APIs, making it easier to connect an LLM with real-time data and enhance its capabilities with up-to-date information.\n",
      "\n",
      "5. **State Management**: It handles state management for conversational applications, keeping track of the dialogue context across interactions, which is crucial for building chatbots or virtual assistants.\n",
      "\n",
      "6. **Testing and Evaluation Tools**: LangChain includes built-in tools for testing and evaluating LLM applications, enabling developers to assess the model's performance and make necessary adjustments more efficiently.\n",
      "\n",
      "7. **Community and Ecosystem**: Being a part of a growing community, developers can benefit from shared resources, examples, and support, which accelerates the development process.\n",
      "\n",
      "8. **Scalability**: LangChain is designed to be scalable, allowing developers to build small prototypes and gradually expand them into more comprehensive applications as needed.\n",
      "\n",
      "By providing these features and utilities, LangChain reduces the complexity usually associated with LLM application development, enabling developers to focus more on functionality and user experience rather than the underlying technical challenges.\n"
     ]
    }
   ],
   "source": [
    "#Import packages from LangChain:\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "#Create a prompt template specifying the AI Behaviour:\n",
    "template = \"You are an artificial intelligence assistant, answer the question. {question}\"\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=template\n",
    "    )\n",
    "#Create the LLM:\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"API_KEY\"))\n",
    "print(llm)\n",
    "\n",
    "#Create a chain to integrate the prompt template and LLM using LCEL (LangChain Expression Language):\n",
    "llm_chain = prompt_template | llm \n",
    "llm_chain\n",
    "\n",
    "question = \"How does LangChain make LLM application development easier ?\"\n",
    "print(llm_chain.invoke(question).content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatModels\n",
    "Chat roles : **system**, **human** and **ai**\n",
    "\n",
    "- system\t🎬 The director\tSets the rules and tone (e.g., \"You are a helpful assistant\")\n",
    "- human\t👦 The person talking\tAsks questions or gives instructions\n",
    "- AI (or assistant)\t🤖 The actor / chatbot\tReplies based on what the human said and the system rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages:\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "#Create the LLM:\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"API_KEY\"))\n",
    "\n",
    "#Create the prompte template:\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        #Behaviour's artificial intelligent agent (system):\n",
    "        (\"system\",\"You are a geography expert that returns the colors present in a country flag.\"),\n",
    "        #User's question (user):\n",
    "        (\"human\",\"France\"),\n",
    "        #AI's answer (assistant/ai):\n",
    "        (\"ai\",\"Blue,White,Red\"),\n",
    "        #User's question (user):\n",
    "        (\"human\",\"{country}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm_chain = prompt_template | llm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a geography expert that returns the colors present in a country flag.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='France'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Blue, White, Red'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='{country}'), additional_kwargs={})]) middle=[] last=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x453f82bd0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x453f828d0>, root_client=<openai.OpenAI object at 0x136ad1d30>, root_async_client=<openai.AsyncOpenAI object at 0x136ad2eb0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
      "input_variables=['country'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a geography expert that returns the colors present in a country flag.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='France'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Blue, White, Red'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='{country}'), additional_kwargs={})]\n",
      "Red, White\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# Create the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"API_KEY\"))\n",
    "\n",
    "# Create the prompt template Chat using ChatPromptTemplate (Conversation History)\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a geography expert that returns the colors present in a country flag.\"),\n",
    "    (\"human\", \"France\"),\n",
    "    (\"ai\", \"Blue, White, Red\"),\n",
    "    (\"human\", \"{country}\")\n",
    "])\n",
    "\n",
    "# Build the chain\n",
    "llm_chain = prompt_template | llm\n",
    "print(llm_chain)\n",
    "print(prompt_template)\n",
    "\n",
    "#Asnwer:\n",
    "country = \"Canada\"\n",
    "response = llm_chain.invoke({\"country\": country}).content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "North America includes several countries. The most prominent ones are:\n",
      "\n",
      "1. **United States**\n",
      "2. **Canada**\n",
      "3. **Mexico**\n",
      "\n",
      "Additionally, there are smaller countries in the Caribbean and Central America that are part of North America, such as:\n",
      "\n",
      "4. **Guatemala**\n",
      "5. **Belize**\n",
      "6. **Honduras**\n",
      "7. **El Salvador**\n",
      "8. **Nicaragua**\n",
      "9. **Costa Rica**\n",
      "10. **Panama**\n",
      "\n",
      "Moreover, the Caribbean countries like Cuba, Jamaica, the Bahamas, and others are also considered part of North America.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "#Quick Start with LangChain:\n",
    "llm = ChatOpenAI(api_key=os.getenv(\"API_KEY\"), model=\"gpt-4o-mini\", verbose=True)\n",
    "prompt = \"Please tell me which country belongs to North America ?\"\n",
    "response = llm.invoke(input=prompt, config=None, stop=\"[\\n]\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='You are a specialist in geography. Answer the following question:\\n{question}') middle=[] last=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x4543534d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x454352b50>, root_client=<openai.OpenAI object at 0x136e7dfd0>, root_async_client=<openai.AsyncOpenAI object at 0x136e7db70>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
      "North America is made up of several countries, primarily divided into three regions: \n",
      "\n",
      "1. **Northern America**:\n",
      "   - United States\n",
      "   - Canada\n",
      "   - Mexico\n",
      "\n",
      "2. **Central America**: (considered part of North America geographically)\n",
      "   - Belize\n",
      "   - Costa Rica\n",
      "   - El Salvador\n",
      "   - Guatemala\n",
      "   - Honduras\n",
      "   - Nicaragua\n",
      "   - Panama\n",
      "\n",
      "3. **Caribbean**: (also part of the wider North American region)\n",
      "   - Antigua and Barbuda\n",
      "   - The Bahamas\n",
      "   - Barbados\n",
      "   - Cuba\n",
      "   - Dominica\n",
      "   - Dominican Republic\n",
      "   - Grenada\n",
      "   - Haiti\n",
      "   - Jamaica\n",
      "   - Saint Kitts and Nevis\n",
      "   - Saint Lucia\n",
      "   - Saint Vincent and the Grenadines\n",
      "   - Trinidad and Tobago\n",
      "\n",
      "In total, these countries constitute the nations that make up North America.\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "#Using Pipeline LCEL:\n",
    "template = \"You are a specialist in geography. Answer the following question:\\n{question}\"\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=template\n",
    ")\n",
    "llm_model = ChatOpenAI(api_key=os.getenv(\"API_KEY\"), model=\"gpt-4o-mini\", verbose=True)\n",
    "llm_chain = prompt_template | llm_model\n",
    "print(llm_chain)\n",
    "question2 = {\"question\": \"Please tell me which countries belong to North America?\"}\n",
    "print(llm_chain.invoke(question).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color: Purple (or green, depending on the variety)\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "#Prompt Template\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You have an intelligent agent capable of returning the color of a fruit or vegetable.\"),\n",
    "    (\"human\", \"Banana\"),\n",
    "    (\"ai\", \"Color : Yellow\"),\n",
    "    (\"human\", \"{user_input}\")\n",
    "])\n",
    "\n",
    "#Build the chain\n",
    "llm_model = ChatOpenAI(api_key=os.getenv(\"API_KEY\"), model=\"gpt-4o-mini\")\n",
    "llm_chain = prompt_template | llm_model\n",
    "\n",
    "#Response\n",
    "ai_response = llm_chain.invoke({\"user_input\":\"Grapes\"}).content\n",
    "print(ai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot prompting\n",
    "\n",
    "Few-shot prompting means giving the AI a few examples of how you want it to respond before asking your real question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many fingers do we have ?\n",
      "10\n",
      "\n",
      "Question: How many eyes do we have?\n",
      "2\n",
      "\n",
      "Question: How many ears do we have?\n",
      "2\n",
      "\n",
      "Question: How many heads do we have?\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, FewShotPromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# Create the examples list of dicts\n",
    "examples = [\n",
    "  {\n",
    "    \"question\": \"How many fingers do we have ?\",\n",
    "    \"answer\": \"10\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"How many eyes do we have?\",\n",
    "    \"answer\": \"2\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"How many ears do we have?\",\n",
    "    \"answer\": \"2\"\n",
    "  }\n",
    "]\n",
    "\n",
    "# Complete the prompt for formatting answers\n",
    "example_prompt = PromptTemplate.from_template(\"Question: {question}\\n{answer}\")\n",
    "\n",
    "# Create the few-shot prompt\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "prompt = prompt_template.invoke({\"input\": \"How many heads do we have?\"})\n",
    "print(prompt.text)\n",
    "\n",
    "# Create an OpenAI chat LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"API_KEY\"))\n",
    "\n",
    "# Create and invoke the chain\n",
    "llm_chain = prompt_template | llm\n",
    "print(llm_chain.invoke({\"input\": \"How many heads do we have?\"}).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here’s a suggested itinerary focusing on the top three activities in Paris: the Eiffel Tower, the Louvre Museum, and the Seine River Cruise. This itinerary assumes a two-day visit, giving you ample time to enjoy each attraction and some leisure time in the city.\n",
      "\n",
      "### **Day 1: Eiffel Tower and Seine River Cruise**\n",
      "\n",
      "**Morning:**\n",
      "- **Breakfast:** Start your day with a traditional French breakfast at a nearby café—croissants and café au lait are excellent choices.\n",
      "- **Eiffel Tower:** Arrive early to avoid crowds. Pre-book your tickets online to access the summit. Spend time enjoying the panoramic views of Paris from the top.\n",
      "\n",
      "**Afternoon:**\n",
      "- **Lunch:** Have lunch at one of the cafes near the Eiffel Tower. Consider trying a classic French dish like quiche or ratatouille.\n",
      "- **Leisure Time:** Take a walk along the Champ de Mars or relax on the banks of the Seine.\n",
      "\n",
      "**Evening:**\n",
      "- **Seine River Cruise:** Book a sunset cruise to take advantage of the beautiful evening lighting. Many cruises offer dining options, allowing you to enjoy dinner while taking in the views of illuminated monuments like Notre-Dame and the Louvre.\n",
      "- **Nightcap or Stroll:** After the cruise, either enjoy a drink at a nearby bar or take a leisurely stroll along the Seine to bask in the ambiance.\n",
      "\n",
      "---\n",
      "\n",
      "### **Day 2: Louvre Museum and Montmartre Exploration**\n",
      "\n",
      "**Morning:**\n",
      "- **Breakfast:** Enjoy breakfast at a local boulangerie, sampling fresh pastries and coffee.\n",
      "- **Louvre Museum:** Arrive early to explore one of the world's largest art museums. Pre-book your tickets to avoid long lines. Focus on the key highlights like the Mona Lisa, Venus de Milo, and the incredible galleries.\n",
      "\n",
      "**Afternoon:**\n",
      "- **Lunch:** Try a nearby bistro for lunch. You might want to try a classic French onion soup or a crepe.\n",
      "- **Explore the Tuileries Garden:** After the Louvre, take a leisurely stroll through the Tuileries Garden, located right next to the museum, and enjoy the beautiful landscape and sculptures.\n",
      "- **Montmartre:** Head to the Montmartre district. Explore the charming streets, visit the Basilica of Sacré-Cœur, and enjoy the views of the city from the top.\n",
      "\n",
      "**Evening:**\n",
      "- **Dinner in Montmartre:** Dine at a local restaurant in Montmartre. You’ll find a range of options, from traditional French cuisine to more modern takes.\n",
      "- **Moulin Rouge:** If you’re in the mood for entertainment, consider ending your day with a cabaret show at the Moulin Rouge (Note: be sure to book tickets in advance).\n",
      "\n",
      "---\n",
      "\n",
      "### Tips for Your Visit:\n",
      "- **Advance Booking:** For popular attractions like the Eiffel Tower and the Louvre, make sure to book your tickets in advance to avoid long waiting times.\n",
      "- **Public Transport:** Use the Metro for easy access to various parts of the city. Consider getting a Paris Visite pass for unlimited travel on public transportation.\n",
      "- **Staying Hydrated:** Be sure to drink plenty of water and keep hydrated, particularly if you’re doing a lot of walking.\n",
      "\n",
      "This itinerary gives you a balance of iconic landmarks, local culture, and leisure time, ensuring a memorable experience in Paris! Enjoy your trip!\n"
     ]
    }
   ],
   "source": [
    "#Let's plan an itinary with LangChain:\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableMap\n",
    "\n",
    "#Sequential prompts:\n",
    "destination_prompt = PromptTemplate(\n",
    "    input_variables = [\"destination\"],\n",
    "    template = \"I'm going on vacation to {destination}, can you suggest some activites out there ?\"\n",
    "    )\n",
    "activities_prompt = PromptTemplate(\n",
    "    input_variables = [\"activities\"],\n",
    "    template = \"Please, create an itinary for the top three activities : {activities} \"\n",
    ")\n",
    "#LLM Model:\n",
    "llm_model = ChatOpenAI(api_key=os.getenv(\"API_KEY\"), model=\"gpt-4o-mini\")\n",
    "\n",
    "####Sequential Chain:\n",
    "\n",
    "#Build the chain:\n",
    "get_activities = destination_prompt | llm_model | StrOutputParser()\n",
    "\n",
    "seq_chain = (\n",
    "    RunnableMap({\"activities\": get_activities})\n",
    "    | activities_prompt\n",
    "    | llm_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 4️⃣ Run the chain\n",
    "result = seq_chain.invoke({\"destination\": \"Paris\"})\n",
    "print(result)\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to LangChain Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain agents are a powerful feature that enable large language models (LLMs) to make decisions and take actions based on dynamic reasoning and tool use. If you're just getting started, here's a beginner-friendly introduction to what LangChain agents are, how they work, and when to use them.\n",
    "\n",
    "🧠 **What is a LangChain Agent ?**\n",
    "- An agent in LangChain is a component that:\n",
    "- Receives a user query\n",
    "- Thinks step by step about how to respond\n",
    "- Chooses tools (APIs, functions, databases, etc.) to call\n",
    "- Executes tools as needed\n",
    "- Returns the final answer to the user\n",
    "\n",
    "It acts almost like an AI assistant that can use tools to accomplish goals, similar to how a human might use a calculator, a search engine, or a file system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReAct (Reason + Act) agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an intelligent agent, my behavior involves processing information, learning continuously, and providing accurate answers to the best of my ability. I strive to assist users in getting the information they need efficiently and reliably.\n",
      "\n",
      "The biggest animal on earth is the blue whale. It can grow up to 100 feet long and weigh as much as 200 tons.\n"
     ]
    }
   ],
   "source": [
    "#Calling a simple AI agent to answer user's prompt:\n",
    "from langchain import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "#Assisant's behaviour:\n",
    "template = \"You are an intelligent agent capable of answering any questions. {question}\"\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=template\n",
    ")\n",
    "#LLM model:\n",
    "llm_model = ChatOpenAI(api_key=os.getenv(\"API_KEY\"), \n",
    "                       model=\"gpt-4\")\n",
    "#Chain:\n",
    "llm_chain = prompt_template | llm_model |StrOutputParser()\n",
    "\n",
    "#Prompt:\n",
    "\n",
    "print(llm_chain.invoke(input=\"what is your behaviour and What is the biggest animal on earth ?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
