{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing AI System\n",
    "\n",
    "### Chatbot\n",
    "\n",
    "A **chatbot** is a computer program that simulates human conversation, usually through text or voice. With **OpenAI**, a chatbot uses powerful language models like **GPT-4** to understand questions and generate smart, human-like answers.\n",
    "\n",
    "In simple terms:\n",
    "\n",
    "> A chatbot with OpenAI can chat, answer questions, summarize info, or help with tasks — almost like talking to a helpful assistant.\n",
    "\n",
    "It works by sending messages to the model (input) and receiving a response (output), often using the `chat.completions.create()` function from the OpenAI API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexBot is ready. Tape 'exit' to quit the conversation\n",
      "ChniniBot : Salut ! Comment puis-je vous aider aujourd'hui ?\n",
      "--------------------------------------------------\n",
      "Thanks for choosing AlexBot. See you soon !\n"
     ]
    }
   ],
   "source": [
    "#This version of chatbot can only be running through the console,\n",
    "#Check code app.py a version of this chatbot deployed on Streamlit Platform for a better experience\n",
    "#To run the chatbot --> Terminal : streamlit run app.py\n",
    "\n",
    "import os \n",
    "import openai\n",
    "from time import sleep\n",
    "from tqdm import tgrange, tqdm_notebook\n",
    "\n",
    "#Bloc\n",
    "MESSAGES = [{\"role\":\"system\", \"content\": \"You're a Chatbot, your name is AlexBot you've been created by Alexandre Ohayon, you can answer any customer's questions by summarizing your answer.\"}]\n",
    "#Dialog Box ask your question to the ChatBot:\n",
    "\n",
    "try:\n",
    "    print(\"AlexBot is ready. Tape 'exit' to quit the conversation\")\n",
    "\n",
    "    #Start a session:\n",
    "    while True:\n",
    "        #Enter your question:\n",
    "        USER_PROMPT = input(\"Enter your message here: \") #Use the dialog box\n",
    "        #Tape exist in lower character to quit the conversation with the Bot.\n",
    "        if USER_PROMPT.lower() == \"exit\": \n",
    "            print(\"Thanks for choosing AlexBot. See you soon !\") #Message from system\n",
    "            break #Session closed\n",
    "            \n",
    "        #If a conversation exists then add user question to the Bloc in line 7:\n",
    "        MESSAGES.append({\"role\":\"user\", \"content\": USER_PROMPT}) #Add the question into the list:\n",
    "        user_question = {\"role\":\"user\", \"content\": USER_PROMPT}\n",
    "        #print(f\"User_question : {USER_PROMPT}\")\n",
    "\n",
    "                \n",
    "        #Pass MESSAGES through the LLM to get response from the AI Agent:\n",
    "        client = openai.OpenAI(organization=None,\n",
    "                                project=None, \n",
    "                                timeout=60*5, #5 minutes and break after that\n",
    "                                max_retries=2,\n",
    "                                api_key=API_KEY)\n",
    "            \n",
    "        #Response from the Intelligent Agent:\n",
    "        # Add the appropriate parameters to the decorator\n",
    "        response = client.chat.completions.create(\n",
    "            model='gpt-4o-mini',\n",
    "            messages=MESSAGES,\n",
    "            max_tokens=300,\n",
    "            temperature=0,\n",
    "            top_p=0.9,\n",
    "            stop=[\"\\n\"]\n",
    "        )\n",
    "\n",
    "        assistant_dict = {\"role\":\"assistant\", \"content\": response.choices[0].message.content}\n",
    "        print(f\"ChniniBot : {assistant_dict[\"content\"]}\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "        #Add the question into the list:\n",
    "        MESSAGES.append(assistant_dict)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error :\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get response in Json Format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"plats\": [\n",
      "    { \"nom\": \"Sushi\", \"pays\": \"Japon\" },\n",
      "    { \"nom\": \"Pizza\", \"pays\": \"Italie\" },\n",
      "    { \"nom\": \"Tacos\", \"pays\": \"Mexique\" },\n",
      "    { \"nom\": \"Croissant\", \"pays\": \"France\" },\n",
      "    { \"nom\": \"Paella\", \"pays\": \"Espagne\" },\n",
      "    { \"nom\": \"Curry\", \"pays\": \"Inde\" },\n",
      "    { \"nom\": \"Poutine\", \"pays\": \"Canada\" },\n",
      "    { \"nom\": \"Hamburger\", \"pays\": \"États-Unis\" },\n",
      "    { \"nom\": \"Falafel\", \"pays\": \"Moyen-Orient\" },\n",
      "    { \"nom\": \"Kimchi\", \"pays\": \"Corée du Sud\" }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Initialisation du client OpenAI avec la clé API (via variable d'environnement)\n",
    "client = openai.OpenAI(organization=None,\n",
    "                                project=None, \n",
    "                                timeout=60*5, #5 minutes and break after that\n",
    "                                max_retries=2,\n",
    "                                api_key=API_KEY) # Remplace si besoin\n",
    "\n",
    "def get_famous_dishes_by_country():\n",
    "    # Prompt demandé\n",
    "    prompt = (\n",
    "        \"Donne-moi une liste des 10 plats les plus connus dans le monde, \"\n",
    "        \"triée par pays, au format JSON. Pour chaque plat, indique le nom du plat et le pays d'origine. \"\n",
    "        \"Utilise ce format : \"\n",
    "        '{ \"plats\": [ { \"nom\": \"Sushi\", \"pays\": \"Japon\" }, ... ] }'\n",
    "    )\n",
    "\n",
    "    # Appel à l'API avec la méthode chat.completions.create\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # ou \"gpt-4\"\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Tu es un assistant culinaire qui fournit des réponses en JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        max_tokens=500,\n",
    "        response_format={\"type\":\"json_object\"} # Nécessite GPT-4-turbo ou GPT-4o\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    json_result = get_famous_dishes_by_country()\n",
    "    print(json_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function tools \n",
    "\n",
    "Function tools (also called tools or function calling in OpenAI) let you connect a chatbot to external functions in your code.\n",
    "\n",
    "🔧 In simple terms:\n",
    "Function tools allow the chatbot to call real functions — like searching a database, getting the weather, or running a calculation — based on the user's question.\n",
    "\n",
    "📌 Example:\n",
    "If the user says:\n",
    "\n",
    "\"What's the weather in Paris?\"\n",
    "\n",
    "The model can call your get_weather(city) function, get the result, and reply with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Job Offer:\n",
      " **Job Title: Data Scientist**\n",
      "\n",
      "**Location:** Montreal, QC, Canada  \n",
      "**Company:** Microsoft  \n",
      "**Salary:** CAD 120,000 per year\n",
      "\n",
      "**Job Description:**\n",
      "\n",
      "Microsoft is seeking a skilled and enthusiastic Data Scientist to join our dynamic team in Montreal. As a Data Scientist, you will play a pivotal role in leveraging data-driven insights to drive strategic decision-making and contribute to the development of innovative solutions across various projects.\n",
      "\n",
      "**Key Responsibilities:**\n",
      "\n",
      "- Analyze large and complex data sets to identify trends, patterns, and actionable insights.\n",
      "- Develop predictive models and algorithms to solve business challenges.\n",
      "- Collaborate with cross-functional teams to define data requirements and drive data collection efforts.\n",
      "- Communicate findings and recommendations to stakeholders through clear and compelling data visualizations.\n",
      "- Stay updated with the latest industry trends and technologies in data science and machine learning to ensure continuous improvement.\n",
      "\n",
      "**Qualifications:**\n",
      "\n",
      "- Bachelor’s or Master’s degree in Data Science, Computer Science, Statistics, or a related field.\n",
      "- Proven experience as a Data Scientist or in a similar analytical role.\n",
      "- Strong proficiency in programming languages such as Python or R.\n",
      "- Experience with machine learning frameworks and techniques.\n",
      "- Excellent problem-solving skills and a proactive attitude toward tackling complex challenges.\n",
      "- Ability to work effectively both independently and as part of a collaborative team.\n",
      "\n",
      "**What We Offer:**\n",
      "\n",
      "- Competitive salary of CAD 120,000 per year.\n",
      "- Comprehensive benefits package including health, dental, and vision insurance.\n",
      "- Opportunities for professional growth and development within a leading global technology company.\n",
      "- A dynamic and inclusive work environment that fosters innovation and encourages diversity.\n",
      "\n",
      "If you’re passionate about using data to drive impactful solutions and want to be part of a talented team at Microsoft, we would love to hear from you. Apply now to become a key player in shaping the future of technology with us in Montreal.\n",
      "\n",
      "**How to Apply:**\n",
      "\n",
      "Please submit your resume and a cover letter outlining your experience and suitability for the role through our career portal at [Microsoft Careers](https://careers.microsoft.com/).\n",
      "\n",
      "**Microsoft is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.**\n",
      "\n",
      "------------------------------\n",
      "🧠 Extracted Info:\n",
      "📌 Position      : Data Scientist\n",
      "🏙️  City          : Montreal, QC, Canada\n",
      "💼 Company Name : Microsoft\n",
      "💰 Salary        : CAD 120,000 per year\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "\n",
    "system = \"You're an AI agent capable of analyzing job offers\"\n",
    "prompt = \"Create a simple job offer for a data scientist position in Microsoft in Montreal and salary equal to 120,000$ \"\n",
    "\n",
    "try:\n",
    "    # Création de l'offre\n",
    "    client = openai.OpenAI(api_key=os.getenv(\"API_KEY\"))\n",
    "\n",
    "    response1 = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    job_offer = response1.choices[0].message.content\n",
    "\n",
    "    # Extraction des informations\n",
    "    response2 = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": job_offer}\n",
    "        ],\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"extract_job_info\",\n",
    "                    \"description\": \"Extract key information from a job offer: position, city, salary, and company name.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"job_position\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The job title (e.g., Data Scientist).\"\n",
    "                            },\n",
    "                            \"city\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"City where the job is located.\"\n",
    "                            },\n",
    "                            \"salary\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Salary for the job.\"\n",
    "                            },\n",
    "                            \"company_name\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Company offering the position.\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"job_position\", \"city\", \"salary\", \"company_name\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"extract_job_info\"}}\n",
    "    )\n",
    "\n",
    "    print(\"📄 Job Offer:\\n\", job_offer)\n",
    "    print(\"\\n------------------------------\")\n",
    "\n",
    "    # Extraire les arguments du tool_call\n",
    "    tool_call = response2.choices[0].message.tool_calls[0]\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "    print(\"🧠 Extracted Info:\")\n",
    "    print(f\"📌 Position      : {args['job_position']}\")\n",
    "    print(f\"🏙️  City          : {args['city']}\")\n",
    "    print(f\"💼 Company Name : {args['company_name']}\")\n",
    "    print(f\"💰 Salary        : {args['salary']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"❌ Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LLM Apps with LangChain\n",
    "\n",
    ">What is LangChain and Why Use It?\n",
    "\n",
    "LangChain is a powerful framework for building applications powered by language models like GPT-4. It helps developers connect LLMs with external data (like PDFs, databases, APIs), manage conversation memory, and build complex workflows using tools and agents.\n",
    "\n",
    ">Why use LangChain?\n",
    "\n",
    "Because it simplifies the process of building smart, context-aware applications with LLMs — like chatbots, search engines, or autonomous agents — and makes them more scalable, modular, and production-ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM output:\n",
      "-----------------------\n",
      "LangChain is a framework designed to facilitate the development of applications that utilize large language models (LLMs). It provides a suite of tools and components for building, managing, and deploying AI-driven applications that can leverage the capabilities of these models. LangChain is particularly useful for applications that require natural language understanding, generation, or other language-based tasks.\n",
      "\n",
      "Key features of LangChain include:\n",
      "\n",
      "1. **Modularity**: LangChain allows developers to build applications by combining different components, such as language models, data connectors, prompt templates, and memory management systems.\n",
      "\n",
      "2. **Flexibility**: It supports various LLMs, enabling users to choose the best model for their specific use case, whether it’s OpenAI’s models, Hugging Face models, or others.\n",
      "\n",
      "3. **Ease of Use**: The framework provides abstractions that simplify common tasks, making it easier for developers to create robust applications without needing extensive expertise in machine learning.\n",
      "\n",
      "4. **Integration with External Data Sources**: LangChain supports connecting to and querying external databases or APIs, allowing language models to access-up-to-date information during inference.\n",
      "\n",
      "5. **Chainable Components**: Developers can create chains that combine multiple operations or models, enriching the functionality of their applications.\n",
      "\n",
      "6. **Prompt Management**: LangChain provides tools for managing prompts, making it easier to configure and optimize how language models generate responses.\n",
      "\n",
      "7. **Memory Capabilities**: It includes strategies for managing context and conversation history, which can be crucial for building interactive applications.\n",
      "\n",
      "LangChain is particularly useful for a range of applications, including chatbots, content generation, information retrieval, and more. Its design encourages experimentation and rapid prototyping, making it a popular choice among developers working with AI and natural language processing.\n"
     ]
    }
   ],
   "source": [
    "# pip install langchain\n",
    "# pip install langchain_openai\n",
    "\n",
    "#Import LangChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "#Build an llm\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=os.getenv(\"API_KEY\"),\n",
    "    base_url=None, organization=None, seed=None)\n",
    "\n",
    "prompt=\"What is LangChain ?\"\n",
    "response = llm.invoke(prompt)\n",
    "print(\"\\nLLM output:\")\n",
    "print(f\"-----------------------\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting Hugging Face models\n",
    "\n",
    ">Prompting Hugging Face Models with LangChain: What & Why\n",
    "\n",
    "LangChain supports integration with Hugging Face models, allowing you to use open-source LLMs (like BLOOM, Falcon, or Mistral) in your applications.\n",
    "\n",
    ">What does it mean to prompt Hugging Face models in LangChain?\n",
    "\n",
    "It means you can send inputs (prompts) to Hugging Face-hosted models via LangChain and process the outputs as part of a chain, agent, or tool-based system.\n",
    "\n",
    ">Why use LangChain with Hugging Face models?\n",
    "\n",
    "Because it gives you the flexibility of open-source models while still benefiting from LangChain’s structure — such as chaining logic, memory management, and tool use — without relying solely on OpenAI or closed APIs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (40034441.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[162], line 9\u001b[0;36m\u001b[0m\n\u001b[0;31m    task=\"text-generation\",\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#pip install huggingface_hub\n",
    "#pip install langchain_huggingface\n",
    "#pip install transformers\n",
    "\n",
    "from langchain_huggingface import HuggingFacePipeline \n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id= #Choose a model llm from huggingface hub,\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\":100}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the LLM:\n",
    "llm = ChatOpenAI(model='gpt-4o-mini',\n",
    "                 api_key=os.getenv(\"API_KEY\")\n",
    "                 )\n",
    "#Predict the words following the text in question:\n",
    "prompt = \"Three reasons for using LangChain for LLM application develope\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
